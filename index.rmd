---
title: "Practical Machine Learning Final Project"
author: "Chris van Hasselt"
date: "February 28, 2016"
output: html_document
---

Personal fitness tracking devices from brands such as Fitbit, Jawbone, or Nike  quantify how much of a particular activity the device user performs. But they rarely quantify how well the user performs an activity. 

The aim of this machine learning project is to analyze personal fitness tracking data to determine the quality, rather than simply the quantity, of activity. Activity trackers use miniature accelerometers to track motion in three dimensions. 

The dataset includes data from accelerometers on the belt, forearm, and dumbbell of six participants.  Participants were asked to perform dumbbell lifts correctly and incorrectly in five different ways.  The analysis examines the data to develop a predictive model, and via that model distinguish correct form from incorrect form.

## Approach

Two datasets are provided for this project, a training and a testing set.  The approach used in this is analysis is to partition the training set, with 75% of the training set used for analysis and 25% used for validation of the derived model.  The two datasets are called _training_ and _testing_, respectively.  The _classe_ variable (column 160) from dataset is used to identify whether an activity was performed correctly.  It is used as the outcome variable to develop the training model.  

A second independent dataset, _rawTesting_, is loaded for blind testing of the predictive model.  The _rawTesting_ 
dataset has no actual values for _classe_.


```{r loadData, message=FALSE}
# libraries used
library(caret)
library(Hmisc)

# read in the raw TRAINING data, to be subdivided into two datasets for developing the model.
rawData  <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",header=TRUE,sep=",")
rawTesting  <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",header=TRUE,sep=",")


partitions <- createDataPartition(y=rawData$classe,p=.75,list=FALSE)

training_orig  <- rawData[partitions,]
testing   <- rawData[-partitions,]


```
## Choosing Data

The training dataset has 160 variables, 159 predictor candidate variables and one outcome variable, _classe_.  

Some of the predictor candidates are clearly not useful for prediction, for example the _X_ variable, essentially a row number, and the _user\_name_ variable.  Neither of these are relevant to whether the activity was performed correctly or not.  Timestamp variables are also not relevant, nor are columns derived from other predictor variables, such as _avg\* and _stddev\*.  All of these variables were removed. Further efforts to reduce the number of predictor variables could be employed to simplify the prediction model.


```{r Select Data, message=FALSE}
# Remove the X, user_name, timestamp*, stdev_*, avg_*, var_*, window*, max*, min*, skewness*, 
# amplitude*, and kurtosis* predictor variables.  
training <- training_orig
training <- training[,-match("X",names(training))]
training <- training[,-match("user_name",names(training))]
training <- training[,-grep("^.+timestamp",names(training))]
training <- training[,-grep("^stddev\\_.+",names(training))]
training <- training[,-grep("^avg\\_.+",names(training))]
training <- training[,-grep("^var\\_.+",names(training))]
training <- training[,-grep("^kurtosis\\_.+",names(training))]
training <- training[,-grep("^min\\_.+",names(training))]
training <- training[,-grep("^max\\_.+",names(training))]
training <- training[,-grep("^skewness\\_.+",names(training))]
training <- training[,-grep("^amplitude\\_.+",names(training))]
training <- training[,-grep("^.+window",names(training))]

```
With 53 variables, creating a feature plot to identify possiblly correlated 
variables proved time consuming.  As demonstated in the principal components 
analysis, it would be useful to see if any variables are correlated. However, 
after identifying the number of closely related variables, it seems apparent
that there are many correlated variables, making prediction difficult.


```{r Correlation, message=FALSE}

# identify correlations within the training set, leaving out the outcome variable.
# choosing .9 as a high level of correlation.
MX <- abs(cor(training[,-grep("^classe",names(training))]))
diag(MX) <- 0 
corVars <- which(MX > .9,arr.ind=TRUE)


length(corVars)

```
With 44 closely related variables, even the simplified dataset retains a lot of 
ambiguity.

## Cross Validation & Model Fitting

A model based on the simplified dataset can be performed using random forest training.  
To train the model with cross validation, I will use the caret package, and 5-fold cross
validation.

```{r Cross Validation, message=FALSE, warning=TRUE}
# setting up cross-validation
set.seed(54321)
trControl <- trainControl(method="cv",number=5,allowParallel=TRUE,verboseIter=TRUE)

# generate model
modelFitRF <- train(training$classe ~ .,method="rf",data=training,trainControl=trControl)

# display characteristics of the model
modelFitRF

modelFitRF$finalModel

```

Using the random forest method, the best results were found with _mtry_=2 and _ntree_=500, the default.

##  Prediction on Testing Data for Validation

Here, I model with the testing data to evaluate the out-of-sample accuracy and error rates. RemeThe _confusionMatrix_ demonstrates the out-of-sample accuracy of 99%, with error less than 1%.

```{r Model Fitting, message=FALSE,cache=TRUE}

# generate predictions for the testing data based random forest model.
predRF <- predict(modelFitRF,newdata=testing)

# compare predicted vs. true values
confusionMatrix(predRF,testing$classe)

```
## Prediction on Unknown Data

The testing dataset is a subset of the known data, validating the computational model. 
The real acid-test is to try the model data where the _classe_ variable is not known, the _rawTesting_ dataset.

As the columns used for model fitting have been reduced, I will apply the same column names to the 
_rawTesting_ dataset, except for the _classe_ column, and then apply the model to get a new set of predictions.

```{r finalPredictions,message=FALSE}
# adjust column names, matching training set -53, the classe column
testUnknown <- rawTesting[,names(training[,-53])]

# predict values for classe based on random forest model
pred_testUnknown <- predict(modelFitRF,newdata=testUnknown)

# and the results are...
pred_testUnknown
```


## Conclusion


The process I've outlined was challenging, but rewarding.  By using a fairly straightforward 
approach, I was able to generate a very accurate model.

