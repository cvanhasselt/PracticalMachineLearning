---
title: "Practical Machine Learning Final Project"
author: "Chris van Hasselt"
date: "February 28, 2016"
output: html_document
---

Personal fitness tracking devices from brands such as Fitbit, Jawbone, or Nike  quantify how much of a particular activity the device user performs. But they rarely quantify how well the user performs an activity. 

The aim of this machine learning project is to analyze personal fitness tracking data to determine the quality, rather than simply the quantity, of activity. Activity trackers use miniature accelerometers to track motion in three dimensions. 

The dataset includes data from accelerometers on the belt, forearm, and dumbbell of six participants.  Participants were asked to perform dumbbell lifts correctly and incorrectly in five different ways.  The analysis examines the data to develop a predictive model, and via that model distinguish correct form from incorrect form.

## Approach

Two datasets are provided for this project, a training and a testing set.  The approach used in this is analysis is to partition the training set, with 75% of the training set used for analysis and 25% used for validation of the derived model.  A second independent dataset is used to test the predictive model.

A second independent dataset is used to test the predictive model.The testing dataset will be analyzed to segregate good form from bad in the dumbbell lift activity.  The _classe_ variable (column 160) from dataset is used to identify whether an activity was performed correctly or not, and is used as the outcome variable to develop the training model.  


```{r loadData, message=FALSE}
# libraries used
library(caret)
library(Hmisc)

# read in the raw TRAINING data, to be subdivided into two datasets for developing the model.
rawData  <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",header=TRUE,sep=",")
rawTesting  <- read.csv("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",header=TRUE,sep=",")

set.seed(201602)
partitions <- createDataPartition(y=rawData$classe,p=.75,list=FALSE)

training_orig  <- rawData[partitions,]
testing   <- rawData[-partitions,]

```
## Choosing Data

The training dataset has 160 variables, 159 predictor candidate variables and one outcome variable, _classe_.  

Some of the predictor candidates are clearly not useful for prediction, for example the _X_ variable, essentially a row number, and the _user\_name_ variable.  Neither of these are relevant to whether the activity was performed correctly or not.  Timestamp variables are also removed.  Columns derived from other predictor variables, such as _avg\* and _stddev\* are also removed.

It would be useful to reduce the number of variables further.  Using the nearZeroVar R function, I've identified all variables that are near zero, and removed those from the training set.  Further efforts to reduce the number of predictor variables could be employed to simplify the prediction model.


```{r selectData, message=FALSE}
# Remove the X, user_name, timestamp*, stdev_*, avg_*, var_*, window*, max*, min*, skewness*, 
# amplitude*, and kurtosis* predictor variables.  
training <- training_orig
training <- training[,-match("X",names(training))]
training <- training[,-match("user_name",names(training))]
training <- training[,-grep("^.+timestamp",names(training))]
training <- training[,-grep("^stddev\\_.+",names(training))]
training <- training[,-grep("^avg\\_.+",names(training))]
training <- training[,-grep("^var\\_.+",names(training))]
training <- training[,-grep("^kurtosis\\_.+",names(training))]
training <- training[,-grep("^min\\_.+",names(training))]
training <- training[,-grep("^max\\_.+",names(training))]
training <- training[,-grep("^skewness\\_.+",names(training))]
training <- training[,-grep("^amplitude\\_.+",names(training))]
training <- training[,-grep("^.+window",names(training))]

```
With 53 variables, creating a feature plot to identify possiblly correlated 
variables was near impossible.  As demonstated in the principal components 
analysis, it would be useful to see if any variables are correlated.


```{r correlation, message=FALSE}

# identify correlations within the training set, leaving out the outcome variable.
# choosing .9 as a high level of correlation.
MX <- abs(cor(training[,-53]))
diag(MX) <- 0 
corVars <- which(MX > .9,arr.ind=TRUE)

length(corVars)

```
With 44 closely related variables, even the simplified dataset retains a lot of 
ambiguity.

## Cross Validation & Model Fitting

A model based on the simplified dataset can be performed using random forest training.  
To train the model with cross validation, I will use the caret package, and 10-fold cross
validation.

```{r cv,message=FALSE}
# setting up cross-validation
trControl <- trainControl(method="cv",number=10)

# generating model
modelFitRF <- train(classe ~ .,data=training,trainControl=trControl,methiod="rf")

predRF <- predict(modFitRF,newdata=testing)

```

## Model Fitting & Prediction

 


```{r message=FALSE,cache=TRUE}

# training using random forests to create a prediction model
modFitRF <- train(classe ~ .,data=training,method="rf")

modFitRF$results

# create a matrix of predictions for the testing data based random forest model.
predRF <- predict(modFitRF,newdata=testing)



```

At this point, I was very confused as to how to proceed further.  When attempting to have R construct a comparison table between the prediction and the actual values in the testing set, I was unable to 
get a comparison to validate that my prediction model was accurate. I believe this is because of the large number of "missed" predictions because of NA values, but am not sure how to proceed.

## Conclusion


The process I've outlined is a first-pass at prediction, a starting point for further exploration.  Given the time constraint for the class it is simply the best I can do with limited time.

